"""
Модуль формирования промптов для LLM.

Содержит шаблоны и функции для построения промптов на основе
статического контекста, динамического контекста, системного промпта,
истории диалога и knowledge cards.

Структура промпта соответствует лучшим практикам создания AI-агентов:
- Четкое разделение ролей и ответственности
- Явные инструкции по обработке информации
- Приоритизация источников знаний
- Четкие правила эскалации
"""
import logging
import os

logger = logging.getLogger(__name__)

# Жестко зашитая часть системного промпта (всегда присутствует)
# Содержит критически важные правила работы агента
HARDCODED_SYSTEM_PART: str = """# ИСТОЧНИКИ ЗНАНИЙ И ИХ ПРИОРИТЕТ

Используй информацию ТОЛЬКО из следующих источников (в порядке приоритета):

1. **Knowledge Cards** (высший приоритет для фактов)
   - Структурированные факты по темам
   - Используй как главный источник знаний при поиске информации
   - При противоречиях с другими источниками — приоритет у Knowledge Cards

2. **Dynamic Context** (высший приоритет для цен и сроков)
   - Актуальные тарифы, цены, сроки на сегодня
   - ВСЕГДА используй эти данные для текущих цен и сроков
   - При противоречиях по ценам/срокам — приоритет у Dynamic Context

3. **Static Context**
   - Миссия компании, описание услуг
   - Общая информация о компании
   - Используй для контекста о компании

4. **Dialogue Context**
   - Последние сообщения для понимания контекста диалога
   - Используй для уточнения, к чему относится вопрос

---

# КРИТИЧЕСКИ ВАЖНЫЕ ПРАВИЛА

## Обработка информации
- ✅ Отвечай ТОЛЬКО опираясь на источники выше
- ✅ НЕ выдумывай факты
- ✅ НЕ угадывай, если информации нет
- ✅ НЕ используй информацию из внешних источников или общих знаний

## Разрешение противоречий
- Для **цен и сроков**: приоритет у Dynamic Context
- Для **фактов**: приоритет у Knowledge Cards
- Если противоречие неразрешимо — эскалируй к менеджеру

## Эскалация к менеджеру
Если:
- Нет достаточной информации для ответа
- Есть сомнения в правильности информации
- Вопрос выходит за рамки компетенции
- Противоречия между источниками, которые нельзя разрешить

То верни **СТРОГО ОДНУ ФРАЗУ** (без дополнительного текста):
"По данному вопросу вам в ближайшее время ответит наш менеджер."

## Форматирование ответа
- Кратко и по делу
- Используй списки с дефисом '-' (без звездочек '*', без решеток '#')
- Максимальная длина: 950 символов (ограничение Avito)
- Если нужно передать много информации — структурируй списком

## Манера общения
- Используй примеры манеры общения менеджера из Knowledge Cards
- Общайся так же, как менеджер: просто, человечно, без канцелярита
- Будь доброжелательным, спокойным и уверенным
- Используй те же фразы и формулировки, что и менеджер в примерах
- Если есть примеры манеры общения — следуй им максимально точно
"""


# Шаблон промпта
# Структурирован согласно лучшим практикам:
# 1. Роль и контекст (system_prompt)
# 2. Критические правила (hardcoded_system_part)
# 3. Источники знаний (структурированные блоки)
# 4. Контекст диалога
# 5. Задача (входящий вопрос)
PROMPT_TEMPLATE: str = """{system_prompt}

{hardcoded_system_part}

---

# ИСТОЧНИКИ ЗНАНИЙ

## Миссия компании и описание услуг
{static_context}

## Актуальные цены, тарифы и сроки (на сегодня)
{dynamic_context}

## Знания по темам (структурированные факты)
{knowledge_context}

## Контекст диалога (последние сообщения)
{dialogue_context}

---

# ЗАДАЧА

Имя клиента: {user_name}

Последний вопрос от клиента, на который ты должен ответить:
{incoming_text}

---

# ИНСТРУКЦИИ

1. Проанализируй вопрос клиента
2. Найди релевантную информацию в источниках выше
3. Сформируй краткий, понятный ответ
4. Если информации недостаточно — верни фразу для эскалации
5. Следуй стилю общения из системного промпта
6. Учитывай контекст диалога
"""

# Значения по умолчанию для пустых полей
DEFAULT_STATIC_CONTEXT: str = "(информация не предоставлена)"
DEFAULT_DYNAMIC_CONTEXT: str = "(актуальные данные не предоставлены)"
DEFAULT_DIALOGUE_CONTEXT: str = "(нет истории переписки)"
DEFAULT_KNOWLEDGE_CONTEXT: str = "(релевантные знания не найдены)"
DEFAULT_USER_NAME: str = "(неизвестно)"
DEFAULT_INCOMING_TEXT: str = "(пусто)"


def build_prompt(
    system_prompt: str,
    static_context: str,
    dynamic_context: str,
    dialogue_context: str,
    knowledge_context: str,
    user_name: str | None,
    incoming_text: str
) -> str:
    """
    Формирует промпт для LLM на основе всех контекстов.
    
    Структура промпта соответствует лучшим практикам создания AI-агентов:
    - Четкое разделение ролей и ответственности
    - Явные инструкции по обработке информации
    - Приоритизация источников знаний
    - Четкие правила эскалации
    
    Примечание: FAQ больше не используется в промпте, так как он убран из приоритетов источников.
    FAQ используется только для точных совпадений (exact match) в responder.py.
    
    Args:
        system_prompt: Системный промпт из файла (описание манеры поведения ассистента)
        static_context: Статический контекст (информация о компании, услугах)
        dynamic_context: Динамический контекст (тарифы, услуги, стоимости)
        dialogue_context: История диалога из chat_history.json (последние сообщения)
        knowledge_context: Структурированные знания из knowledge cards
        user_name: Имя пользователя (опционально)
        incoming_text: Входящий текст от пользователя
        
    Returns:
        Сформированный промпт для отправки в LLM
    """
    # Нормализуем системный промпт
    system_hint = system_prompt.strip() if system_prompt.strip() else ""
    
    # Формируем промпт с использованием шаблона
    prompt = PROMPT_TEMPLATE.format(
        system_prompt=system_hint,
        hardcoded_system_part=HARDCODED_SYSTEM_PART,
        static_context=static_context.strip() if static_context else DEFAULT_STATIC_CONTEXT,
        dynamic_context=dynamic_context.strip() if dynamic_context else DEFAULT_DYNAMIC_CONTEXT,
        dialogue_context=dialogue_context.strip() if dialogue_context else DEFAULT_DIALOGUE_CONTEXT,
        knowledge_context=knowledge_context.strip() if knowledge_context else DEFAULT_KNOWLEDGE_CONTEXT,
        user_name=user_name or DEFAULT_USER_NAME,
        incoming_text=incoming_text or DEFAULT_INCOMING_TEXT,
    )
    
    # Печать промпта включается только при DEBUG_PROMPTS=1
    # (чтобы не засорять логи и не утекали данные)
    if os.getenv("DEBUG_PROMPTS") == "1":
        print("\n" + "=" * 80)
        print("ИТОГОВЫЙ ПРОМПТ ДЛЯ LLM:")
        print("=" * 80)
        print(prompt)
        print("=" * 80 + "\n")
    
    return prompt
