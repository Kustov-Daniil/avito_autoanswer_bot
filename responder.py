"""
–ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM.

–ï–¥–∏–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –∫–∞–∫ –¥–ª—è Avito, —Ç–∞–∫ –∏ –¥–ª—è Telegram.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç FAQ, —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤.
"""
import os
import json
import logging
import difflib
import re
import httpx
from typing import Dict, Any, List, Tuple, Optional
from openai import AsyncOpenAI

from config import (
    LLM_MODEL, TEMPERATURE, OPENAI_API_KEY,
    DATA_DIR, FAQ_PATH, KNOWLEDGE_CARDS_PATH, STATIC_CONTEXT_PATH, DYNAMIC_CONTEXT_PATH, SYSTEM_PROMPT_PATH, CHAT_HISTORY_PATH,
    SIGNAL_PHRASES,
)
from avito_sessions import get_llm_model
from prompts import build_prompt

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_HISTORY_MESSAGES: int = 8
MAX_FAQ_MATCHES: int = 5  # –ö–æ–ª-–≤–æ Q/A –∏–∑ curated FAQ, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø–æ–¥–º–µ—à–∞—Ç—å –≤ –ø—Ä–æ–º–ø—Ç
FAQ_SIMILARITY_CUTOFF: float = 0.50  # –ë–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥ (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π)
FAQ_SIMILARITY_CUTOFF_MIN: float = 0.45  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
FAQ_SIMILARITY_CUTOFF_MAX: float = 0.65  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
FAQ_EXACT_MATCH_THRESHOLD: float = 0.93  # –ü–æ—Ä–æ–≥ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (curated FAQ ‚Üí –º–æ–∂–Ω–æ –æ—Ç–≤–µ—á–∞—Ç—å –±–µ–∑ LLM)
MAX_AVITO_MESSAGE_LENGTH: int = 950  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Avito API

# –í prompt –Ω–µ–ª—å–∑—è –±–µ—Å–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ –≤–ª–∏–≤–∞—Ç—å —Ñ–∞–π–ª—ã: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
MAX_STATIC_CONTEXT_CHARS: int = 8000
MAX_DYNAMIC_CONTEXT_CHARS: int = 8000
MAX_DIALOGUE_CONTEXT_CHARS: int = 3000

# FAQ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ ‚Äúcurated‚Äù (—Ä—É—á–Ω—ã–µ/–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–µ), —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –ø—Ä–æ–º–ø—Ç
FAQ_ALLOWED_SOURCES: set[str] = {"admin", "manager"}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞
if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEY not set! LLM functionality will not work.")
    http_client = None
    client = None
else:
    try:
        http_client = httpx.AsyncClient()
        client = AsyncOpenAI(api_key=OPENAI_API_KEY, http_client=http_client)
        logger.info("OpenAI client initialized successfully with model=%s", LLM_MODEL)
    except Exception as e:
        logger.exception("Failed to initialize OpenAI client: %s", e)
        http_client = None
        client = None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤/–ø–∞–ø–æ–∫
os.makedirs(DATA_DIR, exist_ok=True)
if not os.path.exists(FAQ_PATH):
    with open(FAQ_PATH, "w", encoding="utf-8") as f:
        json.dump([], f, ensure_ascii=False)
if not os.path.exists(KNOWLEDGE_CARDS_PATH):
    with open(KNOWLEDGE_CARDS_PATH, "w", encoding="utf-8") as f:
        json.dump([], f, ensure_ascii=False)
if not os.path.exists(STATIC_CONTEXT_PATH):
    with open(STATIC_CONTEXT_PATH, "w", encoding="utf-8") as f:
        f.write("")
if not os.path.exists(DYNAMIC_CONTEXT_PATH):
    with open(DYNAMIC_CONTEXT_PATH, "w", encoding="utf-8") as f:
        f.write("")
if not os.path.exists(SYSTEM_PROMPT_PATH):
    with open(SYSTEM_PROMPT_PATH, "w", encoding="utf-8") as f:
        f.write("")
if not os.path.exists(CHAT_HISTORY_PATH):
    with open(CHAT_HISTORY_PATH, "w", encoding="utf-8") as f:
        json.dump({}, f, ensure_ascii=False)


def _load_json(path: str, default: Any) -> Any:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JSON —Ñ–∞–π–ª–∞.
    
    Args:
        path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
        default: –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
    Returns:
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError, IOError) as e:
        logger.warning("Failed to load JSON from %s: %s, using default", path, e)
        return default


def _save_json(path: str, data: Any) -> None:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª.
    
    Args:
        path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
        data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except (IOError, OSError) as e:
        logger.error("Failed to save JSON to %s: %s", path, e)
        raise


def _normalize_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
    
    –£–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏, @—É–ø–æ–º–∏–Ω–∞–Ω–∏—è, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (lowercase, –±–µ–∑ —Å—Å—ã–ª–æ–∫, —É–ø–æ–º–∏–Ω–∞–Ω–∏–π, –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)
    """
    if not text:
        return ""
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = text.lower().strip()
    # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ (http://, https://, www.)
    normalized = re.sub(r'https?://\S+|www\.\S+', '', normalized)
    # –£–±–∏—Ä–∞–µ–º @—É–ø–æ–º–∏–Ω–∞–Ω–∏—è
    normalized = re.sub(r'@\w+', '', normalized)
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    normalized = re.sub(r'[^\w\s]', '', normalized)
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    normalized = re.sub(r'\s+', ' ', normalized)
    return normalized.strip()


def _calculate_adaptive_cutoff(text: str) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞.
    
    –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (1-3 —Å–ª–æ–≤–∞) –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥,
    –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ - –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π.
    
    Args:
        text: –í—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.45 - 0.65)
    """
    words = len(text.split())
    
    if words <= 3:
        # –ö–æ—Ä–æ—Ç–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã - –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —É–ª–∞–≤–ª–∏–≤–∞–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π
        return FAQ_SIMILARITY_CUTOFF_MIN
    elif words <= 10:
        # –°—Ä–µ–¥–Ω–∏–µ –≤–æ–ø—Ä–æ—Å—ã - –±–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥
        return FAQ_SIMILARITY_CUTOFF
    else:
        # –î–ª–∏–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        return FAQ_SIMILARITY_CUTOFF_MAX


def _truncate_text(text: str, max_chars: int) -> str:
    if not text:
        return ""
    if len(text) <= max_chars:
        return text
    truncated = text[:max_chars]
    last_space = truncated.rfind(" ")
    if last_space > max_chars - 120:
        truncated = truncated[:last_space]
    return truncated.rstrip() + "..."


def _sanitize_answer(text: str) -> str:
    """
    –õ–µ–≥–∫–∞—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è Avito/Telegram:
    - —É–±–∏—Ä–∞–µ–º markdown-–º–∞—Ä–∫–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ ‚Äú–ª–æ–º–∞—é—Ç‚Äù –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ
    """
    if not text:
        return ""
    out = text.replace("*", "").replace("#", "")
    return out.strip()


def _find_exact_faq_match(incoming_text: str, faq_data: List[Dict[str, str]]) -> Optional[Dict[str, str]]:
    """
    –ò—â–µ—Ç —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ —Å FAQ (‚â•0.9 —Å—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏).
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è curated FAQ: –µ—Å–ª–∏ –º–∞—Ç—á –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π, –º–æ–∂–Ω–æ –æ—Ç–≤–µ—á–∞—Ç—å
    –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –±–∞–∑—ã (–±–µ–∑ LLM), —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏ —Ä–∏—Å–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.
    
    Args:
        incoming_text: –í—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        faq_data: –°–ø–∏—Å–æ–∫ FAQ –∑–∞–ø–∏—Å–µ–π [{"question": str, "answer": str}]
        
    Returns:
        FAQ –∑–∞–ø–∏—Å—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç—å—é ‚â•0.9 –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    """
    if not faq_data or not incoming_text:
        return None
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç
    normalized_incoming = _normalize_text(incoming_text)
    
    best_match = None
    best_score = 0.0
    
    for item in faq_data:
        q = item.get("question", "")
        if not q:
            continue
        
        normalized_q = _normalize_text(q)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarity = difflib.SequenceMatcher(None, normalized_incoming, normalized_q).ratio()
        
        if similarity > best_score:
            best_score = similarity
            best_match = item
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ö–æ–∂–µ—Å—Ç—å ‚â•0.9
    if best_score >= FAQ_EXACT_MATCH_THRESHOLD:
        logger.info(
            "Exact FAQ match found: similarity=%.2f, question='%s'",
            best_score, best_match.get("question", "")[:50] if best_match else ""
        )
        return best_match
    
    return None


def _build_faq_context(incoming_text: str, faq_data: List[Dict[str, str]]) -> str:
    """
    –°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ curated FAQ (—Ä—É—á–Ω—ã–µ/–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞:
    - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (lowercase, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤)
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
    - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –Ω–∞–¥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏ –ø–æ –æ—Ç–≤–µ—Ç–∞–º
    - –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ SequenceMatcher
    
    Args:
        incoming_text: –í—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        faq_data: –°–ø–∏—Å–æ–∫ FAQ –∑–∞–ø–∏—Å–µ–π [{"question": str, "answer": str}]
        
    Returns:
        –°—Ç—Ä–æ–∫–∞ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–∑ FAQ
    """
    if not faq_data or not incoming_text:
        return ""
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç
    normalized_incoming = _normalize_text(incoming_text)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
    adaptive_cutoff = _calculate_adaptive_cutoff(normalized_incoming)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤—Ö–æ–¥—è—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤)
    incoming_words = set(word for word in normalized_incoming.split() if len(word) > 3)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
    questions = []
    question_to_item = {}
    
    for item in faq_data:
        q = item.get("question", "")
        if q:
            normalized_q = _normalize_text(q)
            questions.append(normalized_q)
            question_to_item[normalized_q] = item
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
    matched_questions = difflib.get_close_matches(
        normalized_incoming, questions, n=MAX_FAQ_MATCHES * 2, cutoff=adaptive_cutoff
    )
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    keyword_matches = []
    for q in questions:
        q_words = set(word for word in q.split() if len(word) > 3)
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        common_words = incoming_words & q_words
        if common_words:
            # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—â–∏—Ö —Å–ª–æ–≤
            relevance_score = len(common_words) / max(len(incoming_words), len(q_words))
            if relevance_score >= 0.3:  # –ü–æ—Ä–æ–≥ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                keyword_matches.append((q, relevance_score))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    keyword_matches.sort(key=lambda x: x[1], reverse=True)
    keyword_questions = [q for q, _ in keyword_matches[:MAX_FAQ_MATCHES]]
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç SequenceMatcher, –∑–∞—Ç–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
    all_matched_questions = []
    seen = set()
    
    # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑ SequenceMatcher
    for q in matched_questions:
        if q not in seen:
            all_matched_questions.append(q)
            seen.add(q)
    
    # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for q in keyword_questions:
        if q not in seen and len(all_matched_questions) < MAX_FAQ_MATCHES:
            all_matched_questions.append(q)
            seen.add(q)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    parts = []
    for q in all_matched_questions[:MAX_FAQ_MATCHES]:
        item = question_to_item.get(q)
        if item:
            question = item.get("question", "")
            answer = item.get("answer", "")
            if question and answer:
                parts.append(f"**–í–æ–ø—Ä–æ—Å:** {question}\n**–û—Ç–≤–µ—Ç:** {answer}")
    
    result = "\n\n".join(parts)
    
    if result:
        logger.debug(
            "FAQ context built: incoming_text_length=%d, matches=%d, cutoff=%.2f",
            len(incoming_text), len(parts), adaptive_cutoff
        )
    
    return result


def _build_knowledge_context(incoming_text: str, cards: List[Dict[str, Any]]) -> str:
    """
    –°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ knowledge cards (—Ç–µ–º—ã + —Ñ–∞–∫—Ç—ã), —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤—Ö–æ–¥—è—â–µ–º—É –≤–æ–ø—Ä–æ—Å—É.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    """
    if not cards or not incoming_text:
        return ""

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑ knowledge_cards
    try:
        from utils.knowledge_cards import search_knowledge_cards, update_usage, load_knowledge_cards
        
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ (–∏—Å–∫–ª—é—á–∞—è –º–∞–Ω–µ—Ä—É –æ–±—â–µ–Ω–∏—è - –æ–Ω–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)
        scored_results = search_knowledge_cards(
            incoming_text,
            limit=5,  # –ë–µ—Ä–µ–º —Ç–æ–ø-5 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            min_relevance=0.3
        )
        
        # –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–∞–Ω–µ—Ä—ã –æ–±—â–µ–Ω–∏—è (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∑–∞–ø—Ä–æ—Å–∞)
        # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ—Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Å—Ç–∏–ª—é –æ–±—â–µ–Ω–∏—è
        all_cards = load_knowledge_cards()
        communication_cards_all = [
            c for c in all_cards 
            if c.get("category") == "–º–∞–Ω–µ—Ä–∞_–æ–±—â–µ–Ω–∏—è" and c.get("facts")
        ]
        
        # –ë–µ—Ä–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –º–∞–Ω–µ—Ä—ã –æ–±—â–µ–Ω–∏—è (–¥–æ 5 –∫–∞—Ä—Ç–æ—á–µ–∫)
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–∏–π usage_count –∏–ª–∏ priority
        communication_cards_sorted = sorted(
            communication_cards_all,
            key=lambda c: (
                c.get("priority", 2),  # –°–Ω–∞—á–∞–ª–∞ –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                -c.get("usage_count", 0),  # –ü–æ—Ç–æ–º —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ
                -c.get("relevance_score", 0.5)  # –ü–æ—Ç–æ–º –≤—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            )
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–∞–Ω–µ—Ä—ã –æ–±—â–µ–Ω–∏—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º (–≤—Å–µ–≥–¥–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
        communication_cards_for_context = communication_cards_sorted[:5]
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ
        lines: List[str] = []
        
        # –°–Ω–∞—á–∞–ª–∞ –æ–±—ã—á–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ (—Ç–æ–ø-3)
        if scored_results:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫
            for _, card in scored_results[:3]:
                try:
                    update_usage(card)
                except Exception as e:
                    logger.warning("Failed to update usage for card: %s", e)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–±—ã—á–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ (–Ω–µ –º–∞–Ω–µ—Ä—É –æ–±—â–µ–Ω–∏—è)
            regular_cards = [
                (score, c) for score, c in scored_results
                if c.get("category") != "–º–∞–Ω–µ—Ä–∞_–æ–±—â–µ–Ω–∏—è"
            ]
            
            for score, c in regular_cards[:3]:
                topic = (c.get("topic") or "").strip()
                facts = c.get("facts") or []
                if topic:
                    lines.append(f"**{topic}**")
                if isinstance(facts, list) and facts:
                    for f in facts[:8]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                        f_txt = str(f).strip()
                        if f_txt:
                            lines.append(f"- {f_txt}")
                lines.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        
        # –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–∞–Ω–µ—Ä—ã –æ–±—â–µ–Ω–∏—è (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è —Å—Ç–∏–ª—è)
        if communication_cards_for_context:
            lines.append("**üéØ –ü–†–ò–ú–ï–†–´ –ú–ê–ù–ï–†–´ –û–ë–©–ï–ù–ò–Ø –ú–ï–ù–ï–î–ñ–ï–†–ê (–∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Å—Ç–∏–ª—å):**")
            lines.append("")
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫
            all_examples = []
            for c in communication_cards_for_context:
                facts = c.get("facts") or []
                if isinstance(facts, list) and facts:
                    # –ë–µ—Ä–µ–º –ø–æ 2-3 –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏
                    examples_from_card = [str(f).strip() for f in facts[:3] if f and str(f).strip()]
                    all_examples.extend(examples_from_card)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç)
            for example in all_examples[:10]:  # –ú–∞–∫—Å–∏–º—É–º 10 –ø—Ä–∏–º–µ—Ä–æ–≤
                lines.append(f'üí¨ "{example}"')
            
            lines.append("")
            lines.append("–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –≤ —Ç–æ–º –∂–µ —Å—Ç–∏–ª–µ, —á—Ç–æ –∏ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö –≤—ã—à–µ - –ø—Ä–æ—Å—Ç–æ, —á–µ–ª–æ–≤–µ—á–Ω–æ, –±–µ–∑ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç–∞.")
            lines.append("")
        
        if lines:
            return "\n".join(lines).strip()
        
        return ""
            
    except ImportError:
        logger.warning("Failed to import search_knowledge_cards, using fallback method")
        # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –µ—Å–ª–∏ –Ω–æ–≤—ã–π –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        return _build_knowledge_context_fallback(incoming_text, cards)
    except Exception as e:
        logger.exception("Error in improved knowledge search, using fallback: %s", e)
        return _build_knowledge_context_fallback(incoming_text, cards)


def _build_knowledge_context_fallback(incoming_text: str, cards: List[Dict[str, Any]]) -> str:
    """Fallback –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)."""
    if not cards:
        return ""

    # –†–∞–∑–¥–µ–ª—è–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–∞ –æ–±—ã—á–Ω—ã–µ –∏ –º–∞–Ω–µ—Ä—É –æ–±—â–µ–Ω–∏—è
    regular_cards = []
    communication_cards = []
    
    for c in cards:
        category = c.get("category", "")
        if category == "–º–∞–Ω–µ—Ä–∞_–æ–±—â–µ–Ω–∏—è":
            communication_cards.append(c)
        else:
            regular_cards.append(c)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—ã—á–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏
    normalized_incoming = _normalize_text(incoming_text) if incoming_text else ""
    incoming_words = set(word for word in normalized_incoming.split() if len(word) > 3) if normalized_incoming else set()

    scored: List[Tuple[float, Dict[str, Any]]] = []
    for c in regular_cards:
        topic = c.get("topic", "") or ""
        facts = c.get("facts") or []
        if not topic and not facts:
            continue

        topic_n = _normalize_text(str(topic))
        facts_text = " ".join([str(x) for x in facts]) if isinstance(facts, list) else str(facts)
        facts_n = _normalize_text(facts_text)

        if normalized_incoming:
            s_topic = difflib.SequenceMatcher(None, normalized_incoming, topic_n).ratio() if topic_n else 0.0
            s_facts = difflib.SequenceMatcher(None, normalized_incoming, facts_n).ratio() if facts_n else 0.0
            score = max(s_topic, s_facts)

            card_words = set(word for word in (topic_n + " " + facts_n).split() if len(word) > 3)
            common = incoming_words & card_words
            if common:
                score = min(1.0, score + 0.10 * min(3, len(common)))
        else:
            score = 0.5  # –ï—Å–ª–∏ –Ω–µ—Ç –≤—Ö–æ–¥—è—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞, –¥–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π score

        if score >= 0.35:
            scored.append((score, c))

    scored.sort(key=lambda x: x[0], reverse=True)
    top_regular = [c for _, c in scored[:3]]

    lines: List[str] = []
    
    # –û–±—ã—á–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏
    for c in top_regular:
        topic = (c.get("topic") or "").strip()
        facts = c.get("facts") or []
        if topic:
            lines.append(f"**{topic}**")
        if isinstance(facts, list) and facts:
            for f in facts[:10]:
                f_txt = str(f).strip()
                if f_txt:
                    lines.append(f"- {f_txt}")
        lines.append("")
    
    # –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–∞–Ω–µ—Ä—ã –æ–±—â–µ–Ω–∏—è
    if communication_cards:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
        communication_cards_sorted = sorted(
            communication_cards,
            key=lambda c: (
                c.get("priority", 2),
                -c.get("usage_count", 0),
                -c.get("relevance_score", 0.5)
            )
        )
        
        lines.append("**üéØ –ü–†–ò–ú–ï–†–´ –ú–ê–ù–ï–†–´ –û–ë–©–ï–ù–ò–Ø –ú–ï–ù–ï–î–ñ–ï–†–ê (–∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Å—Ç–∏–ª—å):**")
        lines.append("")
        
        all_examples = []
        for c in communication_cards_sorted[:5]:
            facts = c.get("facts") or []
            if isinstance(facts, list) and facts:
                examples_from_card = [str(f).strip() for f in facts[:3] if f and str(f).strip()]
                all_examples.extend(examples_from_card)
        
        for example in all_examples[:10]:
            lines.append(f'üí¨ "{example}"')
        
        lines.append("")
        lines.append("–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –≤ —Ç–æ–º –∂–µ —Å—Ç–∏–ª–µ, —á—Ç–æ –∏ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö –≤—ã—à–µ - –ø—Ä–æ—Å—Ç–æ, —á–µ–ª–æ–≤–µ—á–Ω–æ, –±–µ–∑ –∫–∞–Ω—Ü–µ–ª—è—Ä–∏—Ç–∞.")
        lines.append("")
    
    return "\n".join(lines).strip() if lines else ""


async def generate_reply(
    dialog_id: str,
    incoming_text: str,
    *,
    user_name: Optional[str] = None,
    save_user_message_to_history: bool = True,
) -> Tuple[Optional[str], Dict[str, bool]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥—è—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
    
    –ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –¥–ª—è Avito –∏ –¥–ª—è Telegram.
    
    –õ–æ–≥–∏–∫–∞:
    - (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) Curated FAQ: –ø—Ä–∏ –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–º –º–∞—Ç—á–µ (>= threshold) –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é
      (–¥–µ—à–µ–≤–ª–µ –∏ –Ω–∞–¥–µ–∂–Ω–µ–µ).
    - –ò–Ω–∞—á–µ: LLM –æ—Ç–≤–µ—á–∞–µ—Ç, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ knowledge cards + dynamic/static + –¥–∏–∞–ª–æ–≥.
    - –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ: LLM –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Å–∏–≥–Ω–∞–ª—å–Ω—É—é —Ñ—Ä–∞–∑—É ‚Üí —ç—Å–∫–∞–ª–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä—É.
    
    Args:
        dialog_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–∏–∞–ª–æ–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "avito_123" –∏–ª–∏ "tg_456")
        incoming_text: –í—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_name: –ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (answer_text, {"contains_signal_phrase": bool})
        - answer_text: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None –µ—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ (–Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∫–ª–∏–µ–Ω—Ç—É)
        - contains_signal_phrase: True –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∏–≥–Ω–∞–ª—å–Ω—É—é —Ñ—Ä–∞–∑—É –æ –ø–µ—Ä–µ–¥–∞—á–µ –º–µ–Ω–µ–¥–∂–µ—Ä—É
    """
    if not incoming_text or not incoming_text.strip():
        logger.warning("generate_reply called with empty incoming_text")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.", {"contains_signal_phrase": False}
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º curated FAQ (—Ä—É—á–Ω—ã–µ/–º–µ–Ω–µ–¥–∂–µ—Ä—Å–∫–∏–µ)
    faq_all = _load_json(FAQ_PATH, [])
    faq_data: List[Dict[str, str]] = []
    if isinstance(faq_all, list):
        for item in faq_all:
            if not isinstance(item, dict):
                continue
            src = (item.get("source") or "").strip().lower()
            if src in FAQ_ALLOWED_SOURCES:
                faq_data.append(item)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Ä–µ–º –î–û —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è)
    chat_history = _load_json(CHAT_HISTORY_PATH, {})
    dialog_history = chat_history.get(dialog_id, [])

    dialog_history_for_context = dialog_history[-MAX_HISTORY_MESSAGES:] if dialog_history else []

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ; Avito webhook —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∞–º)
    if save_user_message_to_history:
        from utils.chat_history import save_user_message
        save_user_message(dialog_id, incoming_text)
        logger.info("Saved user message to chat history for dialog_id=%s", dialog_id)

    # Curated FAQ: –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –ø—Ä—è–º–æ–π –º–∞—Ç—á ‚Äî –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ LLM)
    exact_match = _find_exact_faq_match(incoming_text, faq_data)
    if exact_match:
        faq_answer = _sanitize_answer((exact_match.get("answer") or "").strip())
        faq_answer = _truncate_text(faq_answer, MAX_AVITO_MESSAGE_LENGTH)
        if faq_answer:
            return faq_answer, {"contains_signal_phrase": False}

    logger.info("No exact curated FAQ match, generating answer via LLM for dialog_id=%s", dialog_id)
    
    logger.info("Loaded chat history for dialog_id=%s: %d messages", dialog_id, len(dialog_history))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–º–∞–Ω–µ—Ä–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∏ —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è)
    # –°–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ —Ç–æ–º, –ö–ê–ö –æ—Ç–≤–µ—á–∞—Ç—å, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∞–∫—Ç–æ–≤ –æ –∫–æ–º–ø–∞–Ω–∏–∏
    try:
        with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
            system_prompt = f.read().strip()
    except (FileNotFoundError, IOError) as e:
        logger.warning("Failed to load system prompt: %s", e)
        system_prompt = ""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–º–∏—Å—Å–∏—è –∫–æ–º–ø–∞–Ω–∏–∏, –æ–ø–∏—Å–∞–Ω–∏–µ —É—Å–ª—É–≥)
    # –°–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è —á–∞—Å—Ç–æ: –º–∏—Å—Å–∏—è, –æ–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —É—Å–ª—É–≥
    try:
        with open(STATIC_CONTEXT_PATH, "r", encoding="utf-8") as f:
            static_context = _truncate_text(f.read().strip(), MAX_STATIC_CONTEXT_CHARS)
    except (FileNotFoundError, IOError) as e:
        logger.warning("Failed to load static context: %s", e)
        static_context = ""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã, —Ç–∞—Ä–∏—Ñ—ã, —Å—Ä–æ–∫–∏)
    # –°–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –º–µ–Ω—è–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ: —Ü–µ–Ω—ã, —Å—Ä–æ–∫–∏ –ø–æ–¥–∞—á–∏, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–µ–π
    # –ò–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–ª—è —Ü–µ–Ω –∏ —Å—Ä–æ–∫–æ–≤
    try:
        with open(DYNAMIC_CONTEXT_PATH, "r", encoding="utf-8") as f:
            dynamic_context = _truncate_text(f.read().strip(), MAX_DYNAMIC_CONTEXT_CHARS)
    except (FileNotFoundError, IOError) as e:
        logger.warning("Failed to load dynamic context: %s", e)
        dynamic_context = ""
    
    # –°—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –ë–ï–ó –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    # (–Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –≤ incoming_text)
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: FAQ –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø—Ä–æ–º–ø—Ç–µ, —Ç–æ–ª—å–∫–æ –¥–ª—è exact match –≤—ã—à–µ
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º knowledge cards –∏ —Å—Ç—Ä–æ–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
    try:
        from utils.knowledge_cards import load_knowledge_cards
        knowledge_cards = load_knowledge_cards()
    except ImportError:
        # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
        knowledge_cards = _load_json(KNOWLEDGE_CARDS_PATH, [])
    
    knowledge_context = _build_knowledge_context(
        incoming_text,
        knowledge_cards if isinstance(knowledge_cards, list) else []
    )
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ –≤ —á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ
    dialogue_lines = []
    for m in dialog_history_for_context:
        role = m.get("role", "").capitalize()
        content = m.get("content", "")
        if role and content:
            dialogue_lines.append(f"{role}: {content}")
    dialogue_context = "\n".join(dialogue_lines)
    dialogue_context = _truncate_text(dialogue_context, MAX_DIALOGUE_CONTEXT_CHARS)
    
    logger.info(
        "Built dialogue_context for dialog_id=%s: %d messages, context_length=%d",
        dialog_id, len(dialog_history_for_context), len(dialogue_context)
    )
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
    prompt = build_prompt(
        system_prompt=system_prompt,
        static_context=static_context,
        dynamic_context=dynamic_context,
        dialogue_context=dialogue_context,
        knowledge_context=knowledge_context,
        user_name=user_name,
        incoming_text=incoming_text,
    )
    
    # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å LLM
    current_model = get_llm_model(LLM_MODEL)
    
    logger.info(
        "Calling LLM for dialog_id=%s, model=%s, prompt_length=%d",
        dialog_id, current_model, len(prompt)
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
    if not client:
        logger.error("OpenAI client not initialized! Cannot generate reply for dialog_id=%s", dialog_id)
        return None, {"contains_signal_phrase": True}
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å temperature
        # –î–ª—è gpt-5-mini –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π temperature –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        use_temperature = current_model not in ["gpt-5-mini", "gpt-5"]
        
        logger.info(
            "Creating chat completion: model=%s, use_temperature=%s, temperature=%s",
            current_model, use_temperature, TEMPERATURE if use_temperature else "N/A"
        )
        
        if use_temperature:
            response = await client.chat.completions.create(
                model=current_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=TEMPERATURE,
            )
        else:
            response = await client.chat.completions.create(
                model=current_model,
                messages=[{"role": "user", "content": prompt}],
            )
        
        logger.info("LLM response received: model=%s, choices=%d", response.model, len(response.choices))
        
        if not response.choices or not response.choices[0].message:
            logger.error("LLM response has no choices or message for dialog_id=%s", dialog_id)
            return None, {"contains_signal_phrase": True}
        
        answer = response.choices[0].message.content
        if answer:
            answer = _sanitize_answer(answer.strip())
        else:
            answer = ""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö
        usage_info = {}
        if hasattr(response, 'usage') and response.usage:
            usage_info = {
                "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                "total_tokens": getattr(response.usage, 'total_tokens', 0),
                "model": current_model
            }
            logger.info(
                "Token usage for dialog_id=%s: prompt=%d, completion=%d, total=%d",
                dialog_id, usage_info["prompt_tokens"], usage_info["completion_tokens"], usage_info["total_tokens"]
            )
        
        logger.info("LLM answer extracted: length=%d, preview=%s", len(answer), answer[:100] if answer else "EMPTY")
        
        if not answer:
            logger.warning("LLM returned empty answer for dialog_id=%s", dialog_id)
            # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
            return None, {"contains_signal_phrase": True}
    except Exception as e:
        logger.exception("LLM error for dialog_id=%s: %s", dialog_id, e)
        logger.error("Full error details: type=%s, args=%s", type(e).__name__, e.args)
        # –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
        return None, {"contains_signal_phrase": True}
    
    # –í–∫–ª—é—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö –≤ meta
    meta = {"contains_signal_phrase": False}
    if usage_info:
        meta["usage"] = usage_info
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–∏–≥–Ω–∞–ª—å–Ω–æ–π —Ñ—Ä–∞–∑—ã (–¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ª–æ–≥–∏–∫–∏)
    answer_lower = answer.lower()
    contains_signal = any(phrase in answer_lower for phrase in SIGNAL_PHRASES)
    looks_like_uncertainty = any(
        x in answer_lower
        for x in [
            "–Ω–µ –∑–Ω–∞—é",
            "–Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å",
            "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "–Ω–µ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é",
            "–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö",
        ]
    )
    if looks_like_uncertainty:
        contains_signal = True
    
    # –ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å–∏–≥–Ω–∞–ª—å–Ω–∞—è —Ñ—Ä–∞–∑–∞ - –∑–∞–º–µ–Ω—è–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞
    # –ö–ª–∏–µ–Ω—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –∑–Ω–∞—Ç—å, —á—Ç–æ –µ–≥–æ –ø–µ—Ä–µ–¥–∞—é—Ç –º–µ–Ω–µ–¥–∂–µ—Ä—É
    if contains_signal:
        logger.info("Generated reply contains signal phrase for dialog_id=%s, replacing with client message", dialog_id)
        # –ó–∞–º–µ–Ω—è–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞
        answer = "–ü–æ–¥–æ–∂–¥–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω—è—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
    else:
        answer = _truncate_text(answer, MAX_AVITO_MESSAGE_LENGTH)
    
    # –ù–ï —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é –∑–¥–µ—Å—å - —ç—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ –≤ main.py –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏
    # –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—Ç–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –∫–ª–∏–µ–Ω—Ç—É
    
    # –û–±–Ω–æ–≤–ª—è–µ–º meta —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–∏–≥–Ω–∞–ª—å–Ω–æ–π —Ñ—Ä–∞–∑–µ
    meta["contains_signal_phrase"] = contains_signal
    
    return answer, meta
